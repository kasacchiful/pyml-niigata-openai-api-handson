{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMGRBQgTjNn5/qpN0E2zrml"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# テキスト生成\n","\n","テキスト生成では、以下のようなタスクを行うことができます。\n","\n","- 文章生成\n","- 質問応答\n","- 要約\n","- 翻訳\n","- プログラム生成"],"metadata":{"id":"U7fyHODMir5R"}},{"cell_type":"markdown","source":["## 準備"],"metadata":{"id":"TOFR1KJrjEIX"}},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QPBR74oBibJA","executionInfo":{"status":"ok","timestamp":1699358596825,"user_tz":-540,"elapsed":5394,"user":{"displayName":"Hiroshi Kasahara","userId":"17860752407484945887"}},"outputId":"447d557c-b384-4fd1-816c-0427fca1229b"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: langchain==0.0.331 in /usr/local/lib/python3.10/dist-packages (0.0.331)\n","Requirement already satisfied: openai==0.28.1 in /usr/local/lib/python3.10/dist-packages (0.28.1)\n","Requirement already satisfied: python-dotenv in /usr/local/lib/python3.10/dist-packages (1.0.0)\n","Requirement already satisfied: cohere in /usr/local/lib/python3.10/dist-packages (4.32)\n","Requirement already satisfied: tiktoken in /usr/local/lib/python3.10/dist-packages (0.5.1)\n","Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.331) (6.0.1)\n","Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.331) (2.0.22)\n","Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.331) (3.8.6)\n","Requirement already satisfied: anyio<4.0 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.331) (3.7.1)\n","Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.331) (4.0.3)\n","Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.331) (0.6.1)\n","Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.331) (1.33)\n","Requirement already satisfied: langsmith<0.1.0,>=0.0.52 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.331) (0.0.60)\n","Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.331) (1.23.5)\n","Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.331) (1.10.13)\n","Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.331) (2.31.0)\n","Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.331) (8.2.3)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai==0.28.1) (4.66.1)\n","Requirement already satisfied: backoff<3.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from cohere) (2.2.1)\n","Requirement already satisfied: fastavro==1.8.2 in /usr/local/lib/python3.10/dist-packages (from cohere) (1.8.2)\n","Requirement already satisfied: importlib_metadata<7.0,>=6.0 in /usr/local/lib/python3.10/dist-packages (from cohere) (6.8.0)\n","Requirement already satisfied: urllib3<3,>=1.26 in /usr/local/lib/python3.10/dist-packages (from cohere) (2.0.7)\n","Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2023.6.3)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.331) (23.1.0)\n","Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.331) (3.3.1)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.331) (6.0.4)\n","Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.331) (1.9.2)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.331) (1.4.0)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.331) (1.3.1)\n","Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<4.0->langchain==0.0.331) (3.4)\n","Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<4.0->langchain==0.0.331) (1.3.0)\n","Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<4.0->langchain==0.0.331) (1.1.3)\n","Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain==0.0.331) (3.20.1)\n","Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain==0.0.331) (0.9.0)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.10/dist-packages (from importlib_metadata<7.0,>=6.0->cohere) (3.17.0)\n","Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain==0.0.331) (2.4)\n","Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain==0.0.331) (4.5.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain==0.0.331) (2023.7.22)\n","Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain==0.0.331) (3.0.0)\n","Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.10/dist-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json<0.7,>=0.5.7->langchain==0.0.331) (23.2)\n","Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain==0.0.331) (1.0.0)\n"]}],"source":["## 利用するベースモデルのライブラリ (OpenAI) も別途インストールする\n","!pip install langchain==0.0.331 openai==0.28.1 python-dotenv cohere tiktoken"]},{"cell_type":"code","source":["## Googleドライブをマウント\n","from google.colab import drive\n","drive.mount('./drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"u4BhrM-hjIdl","executionInfo":{"status":"ok","timestamp":1699358602311,"user_tz":-540,"elapsed":5490,"user":{"displayName":"Hiroshi Kasahara","userId":"17860752407484945887"}},"outputId":"109f464e-3034-4ff9-e4b4-37e13ca3a5b3"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at ./drive; to attempt to forcibly remount, call drive.mount(\"./drive\", force_remount=True).\n"]}]},{"cell_type":"code","source":["## 環境変数設定\n","import dotenv\n","dotenv.load_dotenv('./drive/MyDrive/openai.env')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gSpLwKr7jJJf","executionInfo":{"status":"ok","timestamp":1699358602311,"user_tz":-540,"elapsed":7,"user":{"displayName":"Hiroshi Kasahara","userId":"17860752407484945887"}},"outputId":"4f0f56ef-b42f-41dd-c059-c8bfab187973"},"execution_count":3,"outputs":[{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":3}]},{"cell_type":"markdown","source":["## 文章生成"],"metadata":{"id":"oDVvkuRpjgx9"}},{"cell_type":"code","source":["## 入力プロンプトの準備\n","from langchain.prompts import PromptTemplate\n","\n","template = '''\\\n","以下のような頭出しで始まる小説の続きを書いてください。\n","{message}\n","'''\n","prompt = PromptTemplate(template=template, input_variables=[\"message\"])"],"metadata":{"id":"LdaD7FwPjcrC","executionInfo":{"status":"ok","timestamp":1699358602312,"user_tz":-540,"elapsed":6,"user":{"displayName":"Hiroshi Kasahara","userId":"17860752407484945887"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["import os\n","from langchain.llms import OpenAI\n","from langchain.chains import LLMChain\n","\n","## Configuration\n","llm = OpenAI(\n","    model_name=\"text-davinci-003\",\n","    openai_api_key = os.getenv(\"OPENAI_API_KEY\"),  # set OpenAI API Key\n","    temperature = 0.7,  ## サンプリング温度: 0..2 で設定。数値が高い(0.8等)と回答のランダム性が増し、低い(0.2等)と回答がより決定論的になる。\n","    max_tokens = 255,  ## 最大トークン長: 入力トークンと出力トークンの合計の最大値。\n",")\n","\n","## Run\n","message = 'メロスは激怒した。'\n","prompt_text=prompt.format(message=message)\n","res = llm(prompt_text)\n","print(res)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"a8Q9GUKukA8T","executionInfo":{"status":"ok","timestamp":1699358611555,"user_tz":-540,"elapsed":9247,"user":{"displayName":"Hiroshi Kasahara","userId":"17860752407484945887"}},"outputId":"0236038b-891e-4973-c489-5275da0d7e3c"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","「おまえたちのような野蛮人が！　いい加減にしなさい！」\n","メロスは怒りで叫んだ。彼を前にした野蛮人たちは、突然の激しい抗議に驚き、立ち止まった。彼らは何も言わず、ただその場を凝視しているだけだ。\n","\n","メロスは野蛮人たちを睨みつけながら、あることを思い出した。友人のデオードスが言っていた言葉が脳裏をよぎった。「絶対に諦めないで！ 強さと勇気を持っ\n"]}]},{"cell_type":"markdown","source":["## 質問応答"],"metadata":{"id":"wvEX6SeM804_"}},{"cell_type":"code","source":["## 入力プロンプトの準備\n","template = '''\\\n","あなたは、非常に知的な質問回答ボットです。\n","もし、真実に基づいた質問をした場合、あなたは答えを教えます。\n","ナンセンスな質問、トリッキーな質問、明確な答えの無い質問には「不明です」と答えます。\n","\n","Question: {question}\n","Answer:\n","'''\n","prompt = PromptTemplate(template=template, input_variables=[\"question\"])"],"metadata":{"id":"gZYARylO80Dt","executionInfo":{"status":"ok","timestamp":1699358611555,"user_tz":-540,"elapsed":23,"user":{"displayName":"Hiroshi Kasahara","userId":"17860752407484945887"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["## Configuration\n","llm = OpenAI(\n","    model_name=\"text-davinci-003\",\n","    openai_api_key = os.getenv(\"OPENAI_API_KEY\"),  # set OpenAI API Key\n","    temperature = 0.7,\n","    max_tokens = 255,\n",")\n","\n","## Run\n","question = '日本人の平均寿命は何歳ですか。'\n","prompt_text=prompt.format(question=question)\n","res = llm(prompt_text)\n","print(res)"],"metadata":{"id":"7WXk5i-Yn4AK","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1699358612840,"user_tz":-540,"elapsed":1307,"user":{"displayName":"Hiroshi Kasahara","userId":"17860752407484945887"}},"outputId":"cc5a995e-4e09-4c78-bded-ac40a3a760c6"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["日本人の平均寿命は83.09歳です。\n"]}]},{"cell_type":"markdown","source":["## 要約"],"metadata":{"id":"y2xBCQVg_Z81"}},{"cell_type":"code","source":["## 入力プロンプトの準備\n","template = '''\\\n","以下の文章を、小学3年生向けに、わかりやすく要約してください。\n","\n","{text}\n","'''\n","prompt = PromptTemplate(template=template, input_variables=[\"text\"])"],"metadata":{"id":"c7gZjs6j-VEr","executionInfo":{"status":"ok","timestamp":1699358612840,"user_tz":-540,"elapsed":7,"user":{"displayName":"Hiroshi Kasahara","userId":"17860752407484945887"}}},"execution_count":8,"outputs":[]},{"cell_type":"code","source":["## Configuration\n","llm = OpenAI(\n","    model_name=\"text-davinci-003\",\n","    openai_api_key = os.getenv(\"OPENAI_API_KEY\"),  # set OpenAI API Key\n","    temperature = 0.7,\n","    max_tokens = 255,\n",")\n","\n","## Run\n","text = '''『源氏物語』（げんじものがたり）は、平安時代中期に成立した日本の長編物語、小説。文献初出は1008年（寛弘五年）。作者の紫式部にとって生涯で唯一の物語作品である[注 1]。主人公の光源氏を通して、恋愛、栄光と没落、政治的欲望と権力闘争など、平安時代の貴族社会を描いた[1]。\n","下級貴族出身の紫式部は、20代後半で藤原宣孝と結婚し一女をもうけたが、結婚後3年ほどで夫と死別し、その現実を忘れるために物語を書き始めた。これが『源氏物語』の始まりともいわれる[2]。当時、紙は貴重で、紙の提供者がいればその都度書き[注 2]、仲間内で批評し合うなどして楽しんでいたが[注 3]、その物語の評判から藤原道長が娘の中宮彰子の家庭教師として紫式部を呼んだ[注 4]。それを機に宮中に上がった紫式部は、宮仕えをしながら藤原道長の支援の下で物語を書き続け[注 5]、54帖からなる『源氏物語』が完成した[1]。\n","なお、源氏物語は文献初出からおよそ150年後の平安時代末期に「源氏物語絵巻」として絵画化された[4]。現存する絵巻物のうち、徳川美術館と五島美術館所蔵のものは国宝となっている。また現在、『源氏物語』は日本のみならず20か国語を超える翻訳を通じて世界各国で読まれている[注 6]。\n","'''  ## 引用: https://ja.wikipedia.org/wiki/%E6%BA%90%E6%B0%8F%E7%89%A9%E8%AA%9E\n","prompt_text=prompt.format(text=text)\n","res = llm(prompt_text)\n","print(res)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Xvm-xfOW_e2r","executionInfo":{"status":"ok","timestamp":1699358616395,"user_tz":-540,"elapsed":3561,"user":{"displayName":"Hiroshi Kasahara","userId":"17860752407484945887"}},"outputId":"364e145c-b9e8-48bf-ee74-39f7ffbe99e3"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["『源氏物語』は、平安時代中期に紫式部が書いた日本の長編小説です。主人公の光源氏を通して、恋愛、栄光と没落などを描いています。また、現在、20か国語を超える翻訳をして世界で読まれています。\n"]}]},{"cell_type":"markdown","source":["## 翻訳"],"metadata":{"id":"RNXKSpdYBnLr"}},{"cell_type":"code","source":["## 入力プロンプトの準備\n","template = '''\\\n","## 以下の英語の文章を、日本語にギャルっぽく翻訳してください。\n","# English:\n","\n","  {english}\n","\n","# 日本語:\n","'''\n","prompt = PromptTemplate(template=template, input_variables=['english'])"],"metadata":{"id":"ERiRNOI_DWdv","executionInfo":{"status":"ok","timestamp":1699358616395,"user_tz":-540,"elapsed":6,"user":{"displayName":"Hiroshi Kasahara","userId":"17860752407484945887"}}},"execution_count":10,"outputs":[]},{"cell_type":"code","source":["## Configuration\n","llm = OpenAI(\n","    model_name=\"text-davinci-003\",\n","    openai_api_key = os.getenv(\"OPENAI_API_KEY\"),  # set OpenAI API Key\n","    temperature = 0.7,\n","    max_tokens = 255,\n",")\n","\n","## Run\n","english='''\n","I'm so happy!\n","'''\n","prompt_text=prompt.format(english=english)\n","res = llm(prompt_text)\n","print(res)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fAVFxqjONVr8","executionInfo":{"status":"ok","timestamp":1699358616840,"user_tz":-540,"elapsed":450,"user":{"displayName":"Hiroshi Kasahara","userId":"17860752407484945887"}},"outputId":"076eac2f-85fd-44ee-f48f-36c2af05f6a5"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","ワクワクしてぇ♡\n"]}]},{"cell_type":"markdown","source":["### おまけ: ソースコード変換"],"metadata":{"id":"7IilIeDpNYsJ"}},{"cell_type":"code","source":["## 入力プロンプトの準備\n","template = '''\\\n","## Pythonの関数をTypeScriptに変換してください。\n","# Python\n","\n","  {function_code}\n","\n","# TypeScript\n","'''\n","prompt = PromptTemplate(template=template, input_variables=['function_code'])"],"metadata":{"id":"0-5PKx_rBlqL","executionInfo":{"status":"ok","timestamp":1699358616841,"user_tz":-540,"elapsed":7,"user":{"displayName":"Hiroshi Kasahara","userId":"17860752407484945887"}}},"execution_count":12,"outputs":[]},{"cell_type":"code","source":["## Configuration\n","llm = OpenAI(\n","    model_name=\"text-davinci-003\",\n","    openai_api_key = os.getenv(\"OPENAI_API_KEY\"),  # set OpenAI API Key\n","    temperature = 0.7,\n","    max_tokens = 255,\n",")\n","\n","## Run\n","function_code='''\n","def predict_proba(X: Iterable[str]):\n","  return np.array([predict_one_probas(tweet) for tweet in X])\n","'''\n","prompt_text=prompt.format(function_code=function_code)\n","res = llm(prompt_text)\n","print(res)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dw8XyRVUCvYy","executionInfo":{"status":"ok","timestamp":1699358618300,"user_tz":-540,"elapsed":1465,"user":{"displayName":"Hiroshi Kasahara","userId":"17860752407484945887"}},"outputId":"923dc017-0a51-4481-85df-f48ed7f799f0"},"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","function predict_proba(X: Iterable<string>): Array<any> {\n","  return X.map(tweet => predict_one_probas(tweet));\n","}\n"]}]},{"cell_type":"markdown","source":["## プログラム生成"],"metadata":{"id":"Bkzhm5ncEeEA"}},{"cell_type":"code","source":["## 入力プロンプトの準備\n","template = '''\\\n","## 次の仕様を満たす関数を、Pythonで作成してください。\n","# 仕様\n","{spec}\n","\n","# 関数\n","{function_name}({args}):\n","\n","'''\n","prompt = PromptTemplate(template=template, input_variables=['spec', 'function_name', 'args'])"],"metadata":{"id":"dAXWYQVFQiLc","executionInfo":{"status":"ok","timestamp":1699358618300,"user_tz":-540,"elapsed":18,"user":{"displayName":"Hiroshi Kasahara","userId":"17860752407484945887"}}},"execution_count":14,"outputs":[]},{"cell_type":"code","source":["## Configuration\n","llm = OpenAI(\n","    model_name=\"text-davinci-003\",\n","    openai_api_key = os.getenv(\"OPENAI_API_KEY\"),  # set OpenAI API Key\n","    temperature = 0.7,\n","    max_tokens = 255,\n",")\n","\n","## Run\n","spec='''- 入力引数に、1以上の正の整数を受け取ります。\n","- 入力引数の値が3の倍数ならば、文字列 \"Fizz\" を返します。\n","- 入力引数の値が5の倍数ならば、文字列 \"Buzz\" を返します。\n","- 入力引数の値が3の倍数かつ5の倍数ならば、文字列 \"FizzBuzz\" を返します。\n","- 入力引数の値が上記以外の場合、入力引数の値をそのまま返します。\n","'''\n","function_name='fizzbuzz'\n","args='n: Ingeter'\n","\n","prompt_text=prompt.format(spec=spec, function_name=function_name, args=args)\n","res = llm(prompt_text)\n","print(res)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Qs5kNaFqFQIS","executionInfo":{"status":"ok","timestamp":1699358620077,"user_tz":-540,"elapsed":1794,"user":{"displayName":"Hiroshi Kasahara","userId":"17860752407484945887"}},"outputId":"5318a6fa-c808-4bd5-e186-38f9e18763d3"},"execution_count":15,"outputs":[{"output_type":"stream","name":"stdout","text":["    if n % 3 == 0 and n % 5 == 0:\n","        return \"FizzBuzz\"\n","    elif n % 3 == 0:\n","        return \"Fizz\"\n","    elif n % 5 == 0:\n","        return \"Buzz\"\n","    else:\n","        return n\n"]}]},{"cell_type":"markdown","source":["## 他にどんなことができるのか？\n","\n","OpenAIのExampleに、掲載されています。試してみると良いでしょう。\n","\n","- [Examples | OpenAI Platform](https://platform.openai.com/examples)\n","\n","また、以下のサイトにて、Jurassic-2 UltraとOpenAIの比較表があります。比較で試している項目は、OpenAIのExample項目になっていますので、ご参考ください。\n","\n","- [Jurassic-2 Ultra vs OpenAI 徹底比較！！ | moritalous blog](https://moritalous.pages.dev/dbb3d7326909769f26cc#%E3%82%B5%E3%83%9E%E3%83%AA%E3%83%BC)"],"metadata":{"id":"nHKxuCPJLH0t"}},{"cell_type":"markdown","source":["## LangChain Model I/O\n","\n","LangChainが用意しているモジュール。\n","言語モデルのアプリケーションの中核を担う部分。\n","\n","以下の3つで構成される。\n","\n","- Prompts: モデルへ入力をテンプレート化、動的選択、管理を担う\n","- Language models: 共通インタフェースを通じて、言語モデルを呼び出す\n","- Output parsers: モデルの出力から必要な情報を抽出する\n","\n","![Model I/O](https://python.langchain.com/assets/images/model_io-1f23a36233d7731e93576d6885da2750.jpg)\n","\n","https://python.langchain.com/docs/modules/model_io/\n"],"metadata":{"id":"AuoGNqxCMgN1"}},{"cell_type":"markdown","source":["## Prompts\n","\n","言語モデルでのプロンプトとは、ユーザがモデルの応答を誘導するために提供する指示や入力のことです。\n","モデルが文脈を理解し、質問に答えたり、文章を完成させたり、会話に参加したりするような、適切で首尾一貫した言語ベースの出力を生成するのに役立ちます。\n","\n","LangChainでは、プロンプトの生成・操作する上で便利なクラスと関数が用意されています。\n","\n","- Prompt templates: パラメータ化された、プロンプト文字列のテンプレート\n","- Example selectors: プロンプトに含む「例」を動的に選択\n","\n","このノートブックでは、主にPrompt templateを使ってプロンプト文字列を整形してきました。\n","Example selectorsでは、例えば以下のような入力と応答の例をプロンプトに含める際に役に立ちます。\n","(いわゆる few-shot prompting)\n"],"metadata":{"id":"p4uWi74AOaIj"}},{"cell_type":"code","source":["from langchain.prompts import PromptTemplate\n","from langchain.prompts import FewShotPromptTemplate\n","from langchain.prompts.example_selector import LengthBasedExampleSelector\n","\n","# 反対語を作るフリをした、タスクの例\n","examples = [\n","    {\"input\": \"嬉しい\", \"output\": \"悲しい\"},\n","    {\"input\": \"高い\", \"output\": \"低い\"},\n","    {\"input\": \"元気\", \"output\": \"無気力\"},\n","    {\"input\": \"明るい\", \"output\": \"暗い\"},\n","    {\"input\": \"風が強い\", \"output\": \"風が穏やか\"},\n","]\n","\n","example_prompt = PromptTemplate(\n","    input_variables=[\"input\", \"output\"],\n","    template=\"入力: {input}\\n出力: {output}\",\n",")\n","\n","example_selector = LengthBasedExampleSelector(\n","    examples=examples,\n","    example_prompt=example_prompt,\n","    max_length=25,\n",")\n","dynamic_prompt = FewShotPromptTemplate(\n","    example_selector=example_selector,\n","    example_prompt=example_prompt,\n","    prefix=\"各入力の反対語を答えてください。\",\n","    suffix=\"入力: {adjective}\\n出力:\",\n","    input_variables=[\"adjective\"],\n",")"],"metadata":{"id":"kYOqQfymZyas","executionInfo":{"status":"ok","timestamp":1699358620078,"user_tz":-540,"elapsed":13,"user":{"displayName":"Hiroshi Kasahara","userId":"17860752407484945887"}}},"execution_count":16,"outputs":[]},{"cell_type":"code","source":["## LengthBasedExampleSelectorは、入力の文字列が長くなると、例の数を少なくできる。\n","prompt = dynamic_prompt.format(adjective=\"大きい\")\n","print(prompt)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ByN-UeRyb7Bc","executionInfo":{"status":"ok","timestamp":1699358620078,"user_tz":-540,"elapsed":12,"user":{"displayName":"Hiroshi Kasahara","userId":"17860752407484945887"}},"outputId":"384a774b-a8b4-4f40-fe15-16fdb8215b7a"},"execution_count":17,"outputs":[{"output_type":"stream","name":"stdout","text":["各入力の反対語を答えてください。\n","\n","入力: 嬉しい\n","出力: 悲しい\n","\n","入力: 高い\n","出力: 低い\n","\n","入力: 元気\n","出力: 無気力\n","\n","入力: 明るい\n","出力: 暗い\n","\n","入力: 風が強い\n","出力: 風が穏やか\n","\n","入力: 大きい\n","出力:\n"]}]},{"cell_type":"code","source":["from langchain.llms import OpenAI\n","\n","## Configuration\n","llm = OpenAI(\n","    model_name=\"text-davinci-003\",\n","    openai_api_key = os.getenv(\"OPENAI_API_KEY\"),  # set OpenAI API Key\n","    temperature = 0.7,\n","    max_tokens = 255,\n",")\n","res = llm(prompt)\n","print(res)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wZLfZk_QcMdx","executionInfo":{"status":"ok","timestamp":1699358620657,"user_tz":-540,"elapsed":587,"user":{"displayName":"Hiroshi Kasahara","userId":"17860752407484945887"}},"outputId":"7c9691ff-7c8b-4666-e6ef-49ecb9c376a0"},"execution_count":18,"outputs":[{"output_type":"stream","name":"stdout","text":[" 小さい\n"]}]},{"cell_type":"markdown","source":["Example selectorでは、他にも入力値に最も似ている(コサイン類似度やn-gramオーバーラップスコアなどで判定)例の組み合わせをプロンプトに選択することもできます。\n","\n","詳細はLangChainのページをご覧ください。\n","\n","https://python.langchain.com/docs/modules/model_io/prompts/example_selectors/"],"metadata":{"id":"wSnBTo1djnJ8"}},{"cell_type":"markdown","source":["## Language models\n","\n","LangChainは2種類のモデルの統合されたインタフェースを提供しています。\n","\n","- LLMs (Large Language Models)\n","  - 文字列を入力とし、文字列を返すモデル\n","- Chat models\n","  - チャットメッセージ文字列のリストを入力とし、チャットメッセージの文字列を返すモデル\n","\n","### LLM vs Chat model: どっちがいいの？\n","\n","LLMとChat modelは若干の違いがあります。\n","\n","LLMは、文字列を入力し、文字列を返します。チャットモデルは、文字列のリストを入力し、文字列を返します。チャットモデルのベースはLLMですが、チャットでのメッセージのやり取りのために特別なチューニングがなされています。また入力はメッセージ文字列のリストを受け取りますが、それには通常発信者のラベル（「システム」「AI」「人間」など）が付けられます。そして、AIのチャットメッセージとしての文字列を返します。GPT-4やAnthropic社のClaude 2はチャットモデルとして実装されています。\n","\n"],"metadata":{"id":"0LT0UnLFLnFb"}},{"cell_type":"markdown","source":["## Output parsers\n","\n","言語モデルは、出力としてテキスト文字列を返します。ただ、場合によっては文字列ではなく構造化されたデータで出力して欲しい場合があります。この時、Output parsersの出番です。\n","\n","Output parsersは言語モデルの出力を構造化するのに役に立つモジュールです。\n","LangChainでは、以下のモジュールが用意されています。\n","\n","- List parser: カンマ区切りの文字列リスト\n","- Datetime parser: Pythonのdatetimeフォーマット\n","- Enum parser: PythonのEnumフォーマット\n","- Auto-fixing parser: 別のOutput parserをラップしていて、最初のOutput parserが失敗した場合、別のLLMを呼び出してエラーを修正することができる。また、誤ったフォーマットで出力された文字列を、フォーマットされた命令と共に言語モデルに渡し、修正を依頼することもできる。\n","- Pydantic (JSON) parser: JSONフォーマット\n","- Retry parser: 出力文字列のフォーマットがおかしい場合、元のプロンプトを使って単純に再試行し、より良い出力フォーマットを得るためのモジュール\n","- Structured output parser: 複数のフィールドを返したい場合に利用するフォーマット\n","- XML parser: XMLフォーマット\n"],"metadata":{"id":"3qm_DgxJPPco"}},{"cell_type":"code","source":["## example: List parser\n","from langchain.output_parsers import CommaSeparatedListOutputParser\n","from langchain.prompts import PromptTemplate\n","from langchain.llms import OpenAI\n","\n","output_parser = CommaSeparatedListOutputParser()\n","\n","format_instructions = output_parser.get_format_instructions()\n","prompt = PromptTemplate(\n","    template=\"5つの{subject}をリスト化してください。\\n{format_instructions}\",\n","    input_variables=[\"subject\"],\n","    partial_variables={\"format_instructions\": format_instructions}\n",")\n","\n","model = OpenAI(temperature=0)\n","\n","_input = prompt.format(subject=\"アイスクリームのフレーバー\")\n","output = model(_input)\n","\n","_output = output_parser.parse(output)\n","print(f\"入力プロンプト:\\n{_input}\\n\\n出力:\\n{_output}\\n\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"127j64mUHMne","executionInfo":{"status":"ok","timestamp":1699358622206,"user_tz":-540,"elapsed":1553,"user":{"displayName":"Hiroshi Kasahara","userId":"17860752407484945887"}},"outputId":"4f4ab6ee-be33-4a02-b55d-384caaa044bf"},"execution_count":19,"outputs":[{"output_type":"stream","name":"stdout","text":["入力プロンプト:\n","5つのアイスクリームのフレーバーをリスト化してください。\n","Your response should be a list of comma separated values, eg: `foo, bar, baz`\n","\n","出力:\n","['ストロベリー', 'チョコレート', 'バニラ', 'マンゴー', 'ピーナッツバター']\n","\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"5SqPMEltTfsC","executionInfo":{"status":"ok","timestamp":1699358622206,"user_tz":-540,"elapsed":4,"user":{"displayName":"Hiroshi Kasahara","userId":"17860752407484945887"}}},"execution_count":19,"outputs":[]}]}